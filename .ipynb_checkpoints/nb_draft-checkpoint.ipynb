{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab918798-a580-4da8-9762-af6480bc0712",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#019EDB ; padding: 10px 0;\">\n",
    "    <center><h1 style=\"color: white; font-weight:bold\">ABSTRACT</h1></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871bb8c4-9b91-4ed0-bfc6-36cec1af22d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c02c169-fc21-40d2-9c76-418c1d41e8aa",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#019EDB ; padding: 10px 0;\">\n",
    "    <center><h1 style=\"color: white; font-weight:bold\">INTRODUCTION</h1></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765e3478-d416-43ee-a6b5-d7f13fd08d86",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#019EDB ; padding: 10px 0;\">\n",
    "    <center><h2 style=\"color: white; font-weight:bold\">Background</h2></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40544ca4-46a8-4b8f-af0f-6f8df86f9663",
   "metadata": {},
   "source": [
    "In an era where data-driven decisions are pivotal to success, political campaigns are no exception. Recognizing the power of big data analytics to shape political strategies, this project leverages the Global Database of Events, Language, and Tone (GDELT) to extract actionable insights for future political leaders in the Philippines. By harnessing GDELT's extensive repository of media events and sentiment analyses, the project aims to distill the most pressing issues and public sentiments across the country. This approach ensures that campaign platforms are not only relevant but also resonant with the electorateâ€™s current concerns and needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924d6346-5b2c-4e5d-97aa-c175b2a5ca5c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#019EDB ; padding: 10px 0;\">\n",
    "    <center><h2 style=\"color: white; font-weight:bold\">Problem Statement</h2></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250cf2e6-f0ac-4b1f-9b8a-099e5ed850a9",
   "metadata": {},
   "source": [
    "In the digital age, the proliferation of disinformation, particularly through orchestrated campaigns via \"troll farms\", has emerged as a critical challenge in political campaigning across the Philippines and Southeast Asia. These deceptive practices not only distort public perceptions but also undermine the integrity of democratic processes by swaying voter opinions with false narratives. The difficulty lies in the ability of political campaigns to both identify and counteract these disinformation efforts effectively while also promoting genuine and fact-based discourse.\n",
    "\n",
    "This project seeks to address the urgent need for sophisticated data-driven strategies that can discern and mitigate the impact of digital disinformation. Utilizing the extensive monitoring capabilities of GDELT to track media events and sentiment, the study aims to equip political leaders with the tools to identify trends and anomalies in public discourse that may indicate the presence of disinformation. By establishing more transparent and factually accurate communication strategies, aspiring political leaders can enhance their campaign platforms, foster a more informed electorate, and strengthen democratic resilience against the corrosive effects of misinformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0b3f35-17f8-4f0b-a8b1-c41fb7df825e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#019EDB ; padding: 10px 0;\">\n",
    "    <center><h2 style=\"color: white; font-weight:bold\">Objective</h2></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73329576-b954-4b6c-93b3-37425aa84175",
   "metadata": {},
   "source": [
    "The goal of this study is to delve deeper into the information provided generously through the GDELT Project and help assist leaders in pivoting their goals and strategies accordingly. \n",
    "The objectives of this is study are as follows: \n",
    "1. asdasd\n",
    "2. asdasd\n",
    "3. asdasd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd2bf64-5e82-46d2-82ed-9c56382a0b35",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#019EDB ; padding: 10px 0;\">\n",
    "    <center><h1 style=\"color: white; font-weight:bold\">DATA SOURCES and DESCRIPTION</h1></center>\n",
    "</div> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee3ebdfb-b5e2-4c71-ac71-7a96b8abc439",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T14:04:37.306458Z",
     "iopub.status.busy": "2024-05-05T14:04:37.305179Z",
     "iopub.status.idle": "2024-05-05T14:04:37.324726Z",
     "shell.execute_reply": "2024-05-05T14:04:37.323385Z",
     "shell.execute_reply.started": "2024-05-05T14:04:37.306393Z"
    }
   },
   "source": [
    "The **GDELT (Global Data of Events, Language, and Tone) Project** is a comprehensive global monitoring system that tracks news content from broadcasters, print media, and websites in numerous languages across almost every country in the world. It employs advanced techniques to identify and extract key elements driving global discourse and events, such as people, locations, organizations, topics, sources, sentiments, numerical data, quotations, images, and events. By continuously processing this immense stream of information on a second-by-second basis, GDELT generates a freely accessible open data platform that enables computational analysis of the world's events, narratives, and societal forces in real-time.\n",
    "\n",
    "GDELT has several [datasets](https://www.gdeltproject.org/data.html). This study specifically uses the **Global Knowledge Graph** (GKG) dataset. GKG enables the representation of the underlying dimensions, geographic patterns, and network structures inherent in global news coverage. It uses sophisticated natural language processing algorithms to compute and encode a  range of codified metadata that captures the latent and contextual aspects of each document. In essence, the GKG interconnects every person, organization, location, numerical data, theme, news source, and event across the globe into a massive unified network. This network captures what is happening worldwide, the associated contexts and involved entities, as well as the sentiments surrounding these events, providing a daily comprehensive view of our global society.\n",
    "\n",
    "The dataset has the following fields:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855ce575-9657-4ca2-b7d3-dd16729f49f7",
   "metadata": {},
   "source": [
    "| Field Name                    | Description |\n",
    "|-------------------------------|-------------|\n",
    "| GKGRECORDID                   | Each GKG record is assigned a globally unique identifier in a date-oriented serial number format. |\n",
    "| V2.1DATE                      | The publication date of the news media used to construct the GKG file, in YYYYMMDDHHMMSS format. |\n",
    "| V2SOURCECOLLECTIONIDENTIFIER  | A numeric identifier specifying the source collection the document came from. |\n",
    "| V2SOURCECOMMONNAME            | A human-friendly identifier of the source of the document. |\n",
    "| V2DOCUMENTIDENTIFIER          | The unique external identifier for the source document. |\n",
    "| V1COUNTS                      | A semicolon-delimited list of counts found in the document, each separated by the pound symbol. |\n",
    "| V2.1COUNTS                    | Similar to V1COUNTS but includes character offsets for each count. |\n",
    "| V1THEMES                      | A semicolon-delimited list of all themes found in the document. |\n",
    "| V2ENHANCEDTHEMES              | Includes all GKG themes referenced in the document along with character offsets. |\n",
    "| V1LOCATIONS                   | A semicolon-delimited list of all locations found in the text, extracted using the Leetaru algorithm. |\n",
    "| V2ENHANCEDLOCATIONS           | Similar to V1LOCATIONS but includes an extra field for character offsets and additional details. |\n",
    "| V1PERSONS                     | A semicolon-delimited list of all person names found in the text. |\n",
    "| V2ENHANCEDPERSONS             | Includes all person names referenced in the document along with character offsets. |\n",
    "| V1ORGANIZATIONS               | A semicolon-delimited list of all company and organization names found in the text. |\n",
    "| V2ENHANCEDORGANIZATIONS       | Includes all organizations/companies referenced in the document along with character offsets. |\n",
    "| V1.5TONE                      | A list of six core emotional dimensions, each recorded as a single precision floating point number. |\n",
    "| V2.1ENHANCEDDATES             | Contains a list of all date references in the document, along with character offsets. |\n",
    "| V2GCAM                        | The Global Content Analysis Measures field, runs an array of content analysis systems over each document. |\n",
    "| V2.1SHARINGIMAGE              | Specifies a \"sharing image\" for each article as specified by news websites. |\n",
    "| V2.1RELATEDIMAGES             | A list of URLs of images deemed most relevant to the core story of the article. |\n",
    "| V2.1SOCIALIMAGEEMBEDS         | A list of URLs of image-based social media posts embedded in articles. |\n",
    "| V2.1SOCIALVIDEOEMBEDS         | A list of URLs of videos embedded in articles from various platforms like YouTube, Vimeo, etc. |\n",
    "| V2.1QUOTATIONS                | Extracts and segments all quoted statements from each article. |\n",
    "| V2.1ALLNAMES                  | Contains a list of all proper names referenced in the document, along with character offsets. |\n",
    "| V2.1AMOUNTS                   | Contains a list of all precise numeric amounts referenced in the document, along with character offsets. |\n",
    "| V2.1TRANSLATIONINFO           | Records provenance information for machine translated documents. |\n",
    "| V2EXTRASXML                   | Reserved to hold special non-standard data applicable to special subsets of the GDELT collection. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7885a3-d2af-4e24-b403-640e6c5379ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T14:04:45.318289Z",
     "iopub.status.busy": "2024-05-05T14:04:45.316912Z",
     "iopub.status.idle": "2024-05-05T14:04:45.325862Z",
     "shell.execute_reply": "2024-05-05T14:04:45.324225Z",
     "shell.execute_reply.started": "2024-05-05T14:04:45.318223Z"
    }
   },
   "source": [
    "<div style=\"background-color:#019EDB ; padding: 10px 0;\">\n",
    "    <center><h1 style=\"color: white; font-weight:bold\">METHODOLOGY</h1></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31eaeaf-acf2-4782-91ac-364c1178d714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae8ae8d6-fb4a-48e2-a456-fea48e75c5e7",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#019EDB ; padding: 10px 0;\">\n",
    "    <center><h1 style=\"color: white; font-weight:bold\">DATA PREPROCESSING</h1></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d0c634-adf2-4af2-a515-45b40ccb0d24",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#019EDB ; padding: 10px 0;\">\n",
    "    <center><h1 style=\"color: white; font-weight:bold\">EXPLORATORY DATA ANALYSIS</h1></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e40322d-53ae-4820-b545-daabac657942",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#019EDB ; padding: 10px 0;\">\n",
    "    <center><h1 style=\"color: white; font-weight:bold\">INSIGHTS</h1></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c20f139-e108-4304-8985-bf91573f922c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#019EDB ; padding: 10px 0;\">\n",
    "    <center><h1 style=\"color: white; font-weight:bold\">RESULTS and DISCUSSIONS</h1></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9bccd6-f6a3-4ba3-95da-e74dcb493a36",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#019EDB ; padding: 10px 0;\">\n",
    "    <center><h1 style=\"color: white; font-weight:bold\">RECOMMENDATIONS</h1></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367dd6e2-9d9c-44b2-a2e4-43e02e350aa3",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#019EDB ; padding: 10px 0;\">\n",
    "    <center><h1 style=\"color: white; font-weight:bold\">SCOPE & LIMITATIONS</h1></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d0e999-2106-4d83-ac82-a8f41a048c33",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#019EDB ; padding: 10px 0;\">\n",
    "    <center><h1 style=\"color: white; font-weight:bold\">CONCLUSION</h1></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec742009-3f51-47f8-ba1e-17f9934a8ce2",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#019EDB ; padding: 10px 0;\">\n",
    "    <center><h1 style=\"color: white; font-weight:bold\">Paula Martinez - Senator</h1></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a6b679-2f5b-400e-bf8f-e027c7fd5b57",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "\n",
    "The GDELT Project. (n.d.). [Web page]. Retrieved from https://www.gdeltproject.org/\n",
    "\n",
    "Channel News Asia. (2022). Paid troll army for hire: Philippines' social media elections influencers. [Web page]. Channel News Asia. Retrieved from https://www.channelnewsasia.com/cna-insider/paid-troll-army-hire-philippines-social-media-elections-influencers-2917556\n",
    "\n",
    "Rappler. (n.d.). Investigating troll farms: What to look out for. [Web page]. Retrieved from https://www.rappler.com/newsbreak/iq/investigating-troll-farms-what-to-look-out-for\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f550349-8f3b-497b-8c92-707430b0e296",
   "metadata": {},
   "source": [
    "# **SCRATCH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "855fd407-a7ad-46e8-af14-bf96de48736e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T15:25:36.227408Z",
     "iopub.status.busy": "2024-05-05T15:25:36.226306Z",
     "iopub.status.idle": "2024-05-05T15:25:36.600425Z",
     "shell.execute_reply": "2024-05-05T15:25:36.599380Z",
     "shell.execute_reply.started": "2024-05-05T15:25:36.227350Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed70a5ad-221a-4463-9529-80332ccbad63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T15:25:38.867531Z",
     "iopub.status.busy": "2024-05-05T15:25:38.866750Z",
     "iopub.status.idle": "2024-05-05T15:25:42.525436Z",
     "shell.execute_reply": "2024-05-05T15:25:42.523324Z",
     "shell.execute_reply.started": "2024-05-05T15:25:38.867475Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "     .builder\n",
    "     .master('local[*]') # Master URL;\n",
    "     .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46858d22-6984-4841-8522-4e616cc92ff9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T15:25:44.840015Z",
     "iopub.status.busy": "2024-05-05T15:25:44.839258Z",
     "iopub.status.idle": "2024-05-05T15:25:54.625545Z",
     "shell.execute_reply": "2024-05-05T15:25:54.623730Z",
     "shell.execute_reply.started": "2024-05-05T15:25:44.839939Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructType, StructField, StringType,\n",
    "IntegerType, FloatType, TimestampType, LongType)\n",
    "import glob\n",
    "# Define the schemaâ€”NOT FINAL; SHOULD REVISIT\n",
    "schema = StructType([\n",
    "    StructField(\"GKGRECORDID\", StringType(), True),\n",
    "    StructField(\"V2.1DATE\", LongType(), True),\n",
    "    StructField(\"V2SOURCECOLLECTIONIDENTIFIER\", IntegerType(), True),\n",
    "    StructField(\"V2SOURCECOMMONNAME\", StringType(), True),\n",
    "    StructField(\"V2DOCUMENTIDENTIFIER\", StringType(), True),\n",
    "    StructField(\"V1COUNTS\", StringType(), True),\n",
    "    StructField(\"V2.1COUNTS\", StringType(), True),\n",
    "    StructField(\"V1THEMES\", StringType(), True),\n",
    "    StructField(\"V2ENHANCEDTHEMES\", StringType(), True),\n",
    "    StructField(\"V1LOCATIONS\", StringType(), True),\n",
    "    StructField(\"V2ENHANCEDLOCATIONS\", StringType(), True),\n",
    "    StructField(\"V1PERSONS\", StringType(), True),\n",
    "    StructField(\"V2ENHANCEDPERSONS\", StringType(), True),\n",
    "    StructField(\"V1ORGANIZATIONS\", StringType(), True),\n",
    "    StructField(\"V2ENHANCEDORGANIZATIONS\", StringType(), True),\n",
    "    StructField(\"V1.5TONE\", FloatType(), True),\n",
    "    StructField(\"V2.1ENHANCEDDATES\", StringType(), True),\n",
    "    StructField(\"V2GCAM\", StringType(), True),\n",
    "    StructField(\"V2.1SHARINGIMAGE\", StringType(), True),\n",
    "    StructField(\"V2.1RELATEDIMAGES\", StringType(), True),\n",
    "    StructField(\"V2.1SOCIALIMAGEEMBEDS\", StringType(), True),\n",
    "    StructField(\"V2.1SOCIALVIDEOEMBEDS\", StringType(), True),\n",
    "    StructField(\"V2.1QUOTATIONS\", StringType(), True),\n",
    "    StructField(\"V2.1ALLNAMES\", StringType(), True),\n",
    "    StructField(\"V2.1AMOUNTS\", StringType(), True),\n",
    "    StructField(\"V2.1TRANSLATIONINFO\", StringType(), True),\n",
    "    StructField(\"V2EXTRASXML\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define the path and file patterns for the first 8 days of August 2019\n",
    "path = '/mnt/data/public/gdeltv2/gkg/'\n",
    "file_pattern = '2019080[1-8]*.gkg.csv'  # Matches days 01 to 08\n",
    "\n",
    "# Use glob to list files matching the pattern\n",
    "files = glob.glob(path + file_pattern)\n",
    "\n",
    "# Read the files into a DataFrame with the specified schema\n",
    "df_gkg = spark.read.csv(files, sep='\\t', schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d9c7d38-c34f-4394-9bad-88937a91c529",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T15:25:56.506143Z",
     "iopub.status.busy": "2024-05-05T15:25:56.504820Z",
     "iopub.status.idle": "2024-05-05T15:26:17.986355Z",
     "shell.execute_reply": "2024-05-05T15:26:17.984885Z",
     "shell.execute_reply.started": "2024-05-05T15:25:56.506081Z"
    }
   },
   "outputs": [],
   "source": [
    "row_count = df_gkg.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78632369-053e-47d9-b25d-f9f132e4bda1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T15:26:17.989379Z",
     "iopub.status.busy": "2024-05-05T15:26:17.988880Z",
     "iopub.status.idle": "2024-05-05T15:26:17.997185Z",
     "shell.execute_reply": "2024-05-05T15:26:17.995909Z",
     "shell.execute_reply.started": "2024-05-05T15:26:17.989336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 1436716 rows.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The dataset has {row_count} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d273ab0c-f570-46b9-b91d-f2d21545a822",
   "metadata": {},
   "source": [
    "This is a sample of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2cef1e4-622f-476f-82df-20f7fdac2209",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T15:26:26.182159Z",
     "iopub.status.busy": "2024-05-05T15:26:26.181464Z",
     "iopub.status.idle": "2024-05-05T15:26:27.051502Z",
     "shell.execute_reply": "2024-05-05T15:26:27.050585Z",
     "shell.execute_reply.started": "2024-05-05T15:26:26.182099Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GKGRECORDID</th>\n",
       "      <th>V2.1DATE</th>\n",
       "      <th>V2SOURCECOLLECTIONIDENTIFIER</th>\n",
       "      <th>V2SOURCECOMMONNAME</th>\n",
       "      <th>V2DOCUMENTIDENTIFIER</th>\n",
       "      <th>V1COUNTS</th>\n",
       "      <th>V2.1COUNTS</th>\n",
       "      <th>V1THEMES</th>\n",
       "      <th>V2ENHANCEDTHEMES</th>\n",
       "      <th>V1LOCATIONS</th>\n",
       "      <th>...</th>\n",
       "      <th>V2GCAM</th>\n",
       "      <th>V2.1SHARINGIMAGE</th>\n",
       "      <th>V2.1RELATEDIMAGES</th>\n",
       "      <th>V2.1SOCIALIMAGEEMBEDS</th>\n",
       "      <th>V2.1SOCIALVIDEOEMBEDS</th>\n",
       "      <th>V2.1QUOTATIONS</th>\n",
       "      <th>V2.1ALLNAMES</th>\n",
       "      <th>V2.1AMOUNTS</th>\n",
       "      <th>V2.1TRANSLATIONINFO</th>\n",
       "      <th>V2EXTRASXML</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20190801173000-0</td>\n",
       "      <td>20190801173000</td>\n",
       "      <td>1</td>\n",
       "      <td>newstoday.com.bd</td>\n",
       "      <td>http://www.newstoday.com.bd/?option=details&amp;ne...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>TAX_WORLDLANGUAGES;TAX_WORLDLANGUAGES_ARABIC;T...</td>\n",
       "      <td>TAX_FNCACT_JUDGES,1022;MEDIA_MSM,296;SOC_SLAVE...</td>\n",
       "      <td>1#Germany#GM#GM#51.5#10.5#GM;1#Chile#CI#CI#-30...</td>\n",
       "      <td>...</td>\n",
       "      <td>wc:345,c1.1:2,c1.4:1,c12.1:29,c12.10:24,c12.12...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1188|112||a richly imagined , engaging and poe...</td>\n",
       "      <td>Man Booker International,95;Edinburgh Universi...</td>\n",
       "      <td>2,previous collections of short,323;3,novels,3...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20190801173000-1</td>\n",
       "      <td>20190801173000</td>\n",
       "      <td>1</td>\n",
       "      <td>idrw.org</td>\n",
       "      <td>http://idrw.org/ms-velpari-takes-over-from-sun...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>TAX_FNCACT;TAX_FNCACT_DIRECTOR;TAX_FNCACT_CHIE...</td>\n",
       "      <td>TAX_FNCACT_CHIEF,275;EDUCATION,430;SOC_POINTSO...</td>\n",
       "      <td>4#Hindustan, India (General), India#IN#IN00#28...</td>\n",
       "      <td>...</td>\n",
       "      <td>wc:168,c12.1:6,c12.10:14,c12.12:5,c12.13:4,c12...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://youtube.com/channel/UChCONU0XnVC2671b7...</td>\n",
       "      <td>None</td>\n",
       "      <td>Sunil Kumar,143;Tejas Division,322;Aircraft Pr...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;PAGE_AUTHORS&gt;By&lt;/PAGE_AUTHORS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20190801173000-2</td>\n",
       "      <td>20190801173000</td>\n",
       "      <td>1</td>\n",
       "      <td>willistonherald.com</td>\n",
       "      <td>https://www.willistonherald.com/news/oil_and_e...</td>\n",
       "      <td>KILL#4000000##2#Colorado, United States#US#USC...</td>\n",
       "      <td>KILL#4000000##2#Colorado, United States#US#USC...</td>\n",
       "      <td>WB_507_ENERGY_AND_EXTRACTIVES;WB_1702_OILFIELD...</td>\n",
       "      <td>WB_507_ENERGY_AND_EXTRACTIVES,25;WB_1702_OILFI...</td>\n",
       "      <td>2#Colorado, United States#US#USCO#39.0646#-105...</td>\n",
       "      <td>...</td>\n",
       "      <td>wc:715,c1.2:6,c1.3:1,c12.1:47,c12.10:88,c12.11...</td>\n",
       "      <td>https://bloximages.chicago2.vip.townnews.com/w...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://youtube.com/channel/UCHR2WhAPYJ6magx0g...</td>\n",
       "      <td>None</td>\n",
       "      <td>Liberty Oilfield Services,26;Tier Four,2357</td>\n",
       "      <td>5,hydraulic fracturing fleets operating,30;23,...</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;PAGE_AUTHORS&gt;Ren&amp;eacute;e Jean rjean@willisto...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        GKGRECORDID        V2.1DATE  V2SOURCECOLLECTIONIDENTIFIER  \\\n",
       "0  20190801173000-0  20190801173000                             1   \n",
       "1  20190801173000-1  20190801173000                             1   \n",
       "2  20190801173000-2  20190801173000                             1   \n",
       "\n",
       "    V2SOURCECOMMONNAME                               V2DOCUMENTIDENTIFIER  \\\n",
       "0     newstoday.com.bd  http://www.newstoday.com.bd/?option=details&ne...   \n",
       "1             idrw.org  http://idrw.org/ms-velpari-takes-over-from-sun...   \n",
       "2  willistonherald.com  https://www.willistonherald.com/news/oil_and_e...   \n",
       "\n",
       "                                            V1COUNTS  \\\n",
       "0                                               None   \n",
       "1                                               None   \n",
       "2  KILL#4000000##2#Colorado, United States#US#USC...   \n",
       "\n",
       "                                          V2.1COUNTS  \\\n",
       "0                                               None   \n",
       "1                                               None   \n",
       "2  KILL#4000000##2#Colorado, United States#US#USC...   \n",
       "\n",
       "                                            V1THEMES  \\\n",
       "0  TAX_WORLDLANGUAGES;TAX_WORLDLANGUAGES_ARABIC;T...   \n",
       "1  TAX_FNCACT;TAX_FNCACT_DIRECTOR;TAX_FNCACT_CHIE...   \n",
       "2  WB_507_ENERGY_AND_EXTRACTIVES;WB_1702_OILFIELD...   \n",
       "\n",
       "                                    V2ENHANCEDTHEMES  \\\n",
       "0  TAX_FNCACT_JUDGES,1022;MEDIA_MSM,296;SOC_SLAVE...   \n",
       "1  TAX_FNCACT_CHIEF,275;EDUCATION,430;SOC_POINTSO...   \n",
       "2  WB_507_ENERGY_AND_EXTRACTIVES,25;WB_1702_OILFI...   \n",
       "\n",
       "                                         V1LOCATIONS  ...  \\\n",
       "0  1#Germany#GM#GM#51.5#10.5#GM;1#Chile#CI#CI#-30...  ...   \n",
       "1  4#Hindustan, India (General), India#IN#IN00#28...  ...   \n",
       "2  2#Colorado, United States#US#USCO#39.0646#-105...  ...   \n",
       "\n",
       "                                              V2GCAM  \\\n",
       "0  wc:345,c1.1:2,c1.4:1,c12.1:29,c12.10:24,c12.12...   \n",
       "1  wc:168,c12.1:6,c12.10:14,c12.12:5,c12.13:4,c12...   \n",
       "2  wc:715,c1.2:6,c1.3:1,c12.1:47,c12.10:88,c12.11...   \n",
       "\n",
       "                                    V2.1SHARINGIMAGE V2.1RELATEDIMAGES  \\\n",
       "0                                               None              None   \n",
       "1                                               None              None   \n",
       "2  https://bloximages.chicago2.vip.townnews.com/w...              None   \n",
       "\n",
       "  V2.1SOCIALIMAGEEMBEDS                              V2.1SOCIALVIDEOEMBEDS  \\\n",
       "0                  None                                               None   \n",
       "1                  None  https://youtube.com/channel/UChCONU0XnVC2671b7...   \n",
       "2                  None  https://youtube.com/channel/UCHR2WhAPYJ6magx0g...   \n",
       "\n",
       "                                      V2.1QUOTATIONS  \\\n",
       "0  1188|112||a richly imagined , engaging and poe...   \n",
       "1                                               None   \n",
       "2                                               None   \n",
       "\n",
       "                                        V2.1ALLNAMES  \\\n",
       "0  Man Booker International,95;Edinburgh Universi...   \n",
       "1  Sunil Kumar,143;Tejas Division,322;Aircraft Pr...   \n",
       "2        Liberty Oilfield Services,26;Tier Four,2357   \n",
       "\n",
       "                                         V2.1AMOUNTS V2.1TRANSLATIONINFO  \\\n",
       "0  2,previous collections of short,323;3,novels,3...                None   \n",
       "1                                               None                None   \n",
       "2  5,hydraulic fracturing fleets operating,30;23,...                None   \n",
       "\n",
       "                                         V2EXTRASXML  \n",
       "0                                               None  \n",
       "1                    <PAGE_AUTHORS>By</PAGE_AUTHORS>  \n",
       "2  <PAGE_AUTHORS>Ren&eacute;e Jean rjean@willisto...  \n",
       "\n",
       "[3 rows x 27 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gkg.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f34f7de-8213-4970-83e6-5aec28b4e0e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T16:20:31.929058Z",
     "iopub.status.busy": "2024-05-05T16:20:31.928322Z",
     "iopub.status.idle": "2024-05-05T16:20:32.130123Z",
     "shell.execute_reply": "2024-05-05T16:20:32.128407Z",
     "shell.execute_reply.started": "2024-05-05T16:20:31.928960Z"
    }
   },
   "outputs": [],
   "source": [
    "df_final = (df_gkg\n",
    "            .withColumn('CountType', F.regexp_extract('V1COUNTS', r'^([^#]+)',\n",
    "                                                      1))\n",
    "            .withColumn('ExpandedLocation',\n",
    "                        F.explode(F.split(\n",
    "                            F.regexp_extract('V1LOCATIONS', \n",
    "                                             r'(^|;)([^#]+#[^#]+#[^#]+)',\n",
    "                                                   0), \";\")))\n",
    "            .withColumn('ExtractedLocation',\n",
    "                        F.regexp_extract('ExpandedLocation',\n",
    "                                         r'#([^#]+)#', 1))\n",
    "            .withColumn('Region',\n",
    "                        F.element_at(F.split(F.col('ExtractedLocation'),\n",
    "                                             ', '), 1)) #Region: first element\n",
    "            .withColumn('Country',\n",
    "                        F.element_at(F.split(F.col('ExtractedLocation'),\n",
    "                                             ', '), -1)) #Country: last element\n",
    "            .withColumn('Region', F.when(F.col('Region') == F.col('Country'),\n",
    "                                         None).otherwise(F.col('Region')))\n",
    "            .withColumn('THEMES_SINGLE',\n",
    "                        F.explode(F.split(F.regexp_extract('V1THEMES',\n",
    "                                                           r'([^;]+)', 0),\n",
    "                                          \";\")))\n",
    "            .drop('ExpandedLocation', 'ExtractedLocation') #Remove  columns\n",
    "            # .persist()\n",
    "           )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa06a46-2830-48c4-ae2c-2d06fbd896f3",
   "metadata": {},
   "source": [
    "The final dataset with exploded count types, locations, and themes:\n",
    "\n",
    "**Note:**\n",
    "- not sure if `persist` makes the succeeding codes slower/faster but iirc it's supposed to optimize how the initial code of `df_final` is executed --> di na niya dinadaanan ulit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5769ac21-d6a9-4f5e-a7c3-eb88aa86b33a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T16:20:56.371027Z",
     "iopub.status.busy": "2024-05-05T16:20:56.370337Z",
     "iopub.status.idle": "2024-05-05T16:20:56.761066Z",
     "shell.execute_reply": "2024-05-05T16:20:56.760328Z",
     "shell.execute_reply.started": "2024-05-05T16:20:56.370968Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GKGRECORDID</th>\n",
       "      <th>V2.1DATE</th>\n",
       "      <th>V2SOURCECOLLECTIONIDENTIFIER</th>\n",
       "      <th>V2SOURCECOMMONNAME</th>\n",
       "      <th>V2DOCUMENTIDENTIFIER</th>\n",
       "      <th>V1COUNTS</th>\n",
       "      <th>V2.1COUNTS</th>\n",
       "      <th>V1THEMES</th>\n",
       "      <th>V2ENHANCEDTHEMES</th>\n",
       "      <th>V1LOCATIONS</th>\n",
       "      <th>...</th>\n",
       "      <th>V2.1SOCIALVIDEOEMBEDS</th>\n",
       "      <th>V2.1QUOTATIONS</th>\n",
       "      <th>V2.1ALLNAMES</th>\n",
       "      <th>V2.1AMOUNTS</th>\n",
       "      <th>V2.1TRANSLATIONINFO</th>\n",
       "      <th>V2EXTRASXML</th>\n",
       "      <th>CountType</th>\n",
       "      <th>Region</th>\n",
       "      <th>Country</th>\n",
       "      <th>THEMES_SINGLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20190801173000-0</td>\n",
       "      <td>20190801173000</td>\n",
       "      <td>1</td>\n",
       "      <td>newstoday.com.bd</td>\n",
       "      <td>http://www.newstoday.com.bd/?option=details&amp;ne...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>TAX_WORLDLANGUAGES;TAX_WORLDLANGUAGES_ARABIC;T...</td>\n",
       "      <td>TAX_FNCACT_JUDGES,1022;MEDIA_MSM,296;SOC_SLAVE...</td>\n",
       "      <td>1#Germany#GM#GM#51.5#10.5#GM;1#Chile#CI#CI#-30...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1188|112||a richly imagined , engaging and poe...</td>\n",
       "      <td>Man Booker International,95;Edinburgh Universi...</td>\n",
       "      <td>2,previous collections of short,323;3,novels,3...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Germany</td>\n",
       "      <td>TAX_WORLDLANGUAGES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20190801173000-1</td>\n",
       "      <td>20190801173000</td>\n",
       "      <td>1</td>\n",
       "      <td>idrw.org</td>\n",
       "      <td>http://idrw.org/ms-velpari-takes-over-from-sun...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>TAX_FNCACT;TAX_FNCACT_DIRECTOR;TAX_FNCACT_CHIE...</td>\n",
       "      <td>TAX_FNCACT_CHIEF,275;EDUCATION,430;SOC_POINTSO...</td>\n",
       "      <td>4#Hindustan, India (General), India#IN#IN00#28...</td>\n",
       "      <td>...</td>\n",
       "      <td>https://youtube.com/channel/UChCONU0XnVC2671b7...</td>\n",
       "      <td>None</td>\n",
       "      <td>Sunil Kumar,143;Tejas Division,322;Aircraft Pr...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;PAGE_AUTHORS&gt;By&lt;/PAGE_AUTHORS&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>Hindustan</td>\n",
       "      <td>India</td>\n",
       "      <td>TAX_FNCACT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20190801173000-2</td>\n",
       "      <td>20190801173000</td>\n",
       "      <td>1</td>\n",
       "      <td>willistonherald.com</td>\n",
       "      <td>https://www.willistonherald.com/news/oil_and_e...</td>\n",
       "      <td>KILL#4000000##2#Colorado, United States#US#USC...</td>\n",
       "      <td>KILL#4000000##2#Colorado, United States#US#USC...</td>\n",
       "      <td>WB_507_ENERGY_AND_EXTRACTIVES;WB_1702_OILFIELD...</td>\n",
       "      <td>WB_507_ENERGY_AND_EXTRACTIVES,25;WB_1702_OILFI...</td>\n",
       "      <td>2#Colorado, United States#US#USCO#39.0646#-105...</td>\n",
       "      <td>...</td>\n",
       "      <td>https://youtube.com/channel/UCHR2WhAPYJ6magx0g...</td>\n",
       "      <td>None</td>\n",
       "      <td>Liberty Oilfield Services,26;Tier Four,2357</td>\n",
       "      <td>5,hydraulic fracturing fleets operating,30;23,...</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;PAGE_AUTHORS&gt;Ren&amp;eacute;e Jean rjean@willisto...</td>\n",
       "      <td>KILL</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>United States</td>\n",
       "      <td>WB_507_ENERGY_AND_EXTRACTIVES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20190801173000-3</td>\n",
       "      <td>20190801173000</td>\n",
       "      <td>1</td>\n",
       "      <td>9news.com</td>\n",
       "      <td>https://www.9news.com/article/life/style/color...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>LEADER;TAX_FNCACT;TAX_FNCACT_PRESIDENT;USPEC_P...</td>\n",
       "      <td>WB_137_WATER,339;LEADER,177;TAX_FNCACT_PRESIDE...</td>\n",
       "      <td>1#United States#US#US#39.828175#-98.5795#US;3#...</td>\n",
       "      <td>...</td>\n",
       "      <td>https://youtube.com/iframe_api?noext;https://y...</td>\n",
       "      <td>None</td>\n",
       "      <td>United States,21;Centennial State,83;President...</td>\n",
       "      <td>20,attractions #x2014,395;41,parks,952;</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;PAGE_LINKS&gt;http://cpw.state.co.us/aboutus/Pag...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>United States</td>\n",
       "      <td>LEADER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20190801173000-4</td>\n",
       "      <td>20190801173000</td>\n",
       "      <td>1</td>\n",
       "      <td>aninews.in</td>\n",
       "      <td>https://www.aninews.in/news/national/general-n...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>GENERAL_GOVERNMENT;EPU_POLICY;EPU_POLICY_GOVER...</td>\n",
       "      <td>GENERAL_HEALTH,232;GENERAL_HEALTH,285;GENERAL_...</td>\n",
       "      <td>4#Delhi, Delhi, India#IN#IN07#28.6667#77.2167#...</td>\n",
       "      <td>...</td>\n",
       "      <td>https://youtube.com/channel/UCtFQDgA8J8_iiwc5-...</td>\n",
       "      <td>None</td>\n",
       "      <td>New Delhi,10;Lieutenant Governor Anil Baijal,1...</td>\n",
       "      <td>5,lakh Delhi government employees,47;</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;PAGE_LINKS&gt;https://www.aninews.in/search&lt;/PAG...</td>\n",
       "      <td>None</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>India</td>\n",
       "      <td>GENERAL_GOVERNMENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        GKGRECORDID        V2.1DATE  V2SOURCECOLLECTIONIDENTIFIER  \\\n",
       "0  20190801173000-0  20190801173000                             1   \n",
       "1  20190801173000-1  20190801173000                             1   \n",
       "2  20190801173000-2  20190801173000                             1   \n",
       "3  20190801173000-3  20190801173000                             1   \n",
       "4  20190801173000-4  20190801173000                             1   \n",
       "\n",
       "    V2SOURCECOMMONNAME                               V2DOCUMENTIDENTIFIER  \\\n",
       "0     newstoday.com.bd  http://www.newstoday.com.bd/?option=details&ne...   \n",
       "1             idrw.org  http://idrw.org/ms-velpari-takes-over-from-sun...   \n",
       "2  willistonherald.com  https://www.willistonherald.com/news/oil_and_e...   \n",
       "3            9news.com  https://www.9news.com/article/life/style/color...   \n",
       "4           aninews.in  https://www.aninews.in/news/national/general-n...   \n",
       "\n",
       "                                            V1COUNTS  \\\n",
       "0                                               None   \n",
       "1                                               None   \n",
       "2  KILL#4000000##2#Colorado, United States#US#USC...   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                          V2.1COUNTS  \\\n",
       "0                                               None   \n",
       "1                                               None   \n",
       "2  KILL#4000000##2#Colorado, United States#US#USC...   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                            V1THEMES  \\\n",
       "0  TAX_WORLDLANGUAGES;TAX_WORLDLANGUAGES_ARABIC;T...   \n",
       "1  TAX_FNCACT;TAX_FNCACT_DIRECTOR;TAX_FNCACT_CHIE...   \n",
       "2  WB_507_ENERGY_AND_EXTRACTIVES;WB_1702_OILFIELD...   \n",
       "3  LEADER;TAX_FNCACT;TAX_FNCACT_PRESIDENT;USPEC_P...   \n",
       "4  GENERAL_GOVERNMENT;EPU_POLICY;EPU_POLICY_GOVER...   \n",
       "\n",
       "                                    V2ENHANCEDTHEMES  \\\n",
       "0  TAX_FNCACT_JUDGES,1022;MEDIA_MSM,296;SOC_SLAVE...   \n",
       "1  TAX_FNCACT_CHIEF,275;EDUCATION,430;SOC_POINTSO...   \n",
       "2  WB_507_ENERGY_AND_EXTRACTIVES,25;WB_1702_OILFI...   \n",
       "3  WB_137_WATER,339;LEADER,177;TAX_FNCACT_PRESIDE...   \n",
       "4  GENERAL_HEALTH,232;GENERAL_HEALTH,285;GENERAL_...   \n",
       "\n",
       "                                         V1LOCATIONS  ...  \\\n",
       "0  1#Germany#GM#GM#51.5#10.5#GM;1#Chile#CI#CI#-30...  ...   \n",
       "1  4#Hindustan, India (General), India#IN#IN00#28...  ...   \n",
       "2  2#Colorado, United States#US#USCO#39.0646#-105...  ...   \n",
       "3  1#United States#US#US#39.828175#-98.5795#US;3#...  ...   \n",
       "4  4#Delhi, Delhi, India#IN#IN07#28.6667#77.2167#...  ...   \n",
       "\n",
       "                               V2.1SOCIALVIDEOEMBEDS  \\\n",
       "0                                               None   \n",
       "1  https://youtube.com/channel/UChCONU0XnVC2671b7...   \n",
       "2  https://youtube.com/channel/UCHR2WhAPYJ6magx0g...   \n",
       "3  https://youtube.com/iframe_api?noext;https://y...   \n",
       "4  https://youtube.com/channel/UCtFQDgA8J8_iiwc5-...   \n",
       "\n",
       "                                      V2.1QUOTATIONS  \\\n",
       "0  1188|112||a richly imagined , engaging and poe...   \n",
       "1                                               None   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                        V2.1ALLNAMES  \\\n",
       "0  Man Booker International,95;Edinburgh Universi...   \n",
       "1  Sunil Kumar,143;Tejas Division,322;Aircraft Pr...   \n",
       "2        Liberty Oilfield Services,26;Tier Four,2357   \n",
       "3  United States,21;Centennial State,83;President...   \n",
       "4  New Delhi,10;Lieutenant Governor Anil Baijal,1...   \n",
       "\n",
       "                                         V2.1AMOUNTS V2.1TRANSLATIONINFO  \\\n",
       "0  2,previous collections of short,323;3,novels,3...                None   \n",
       "1                                               None                None   \n",
       "2  5,hydraulic fracturing fleets operating,30;23,...                None   \n",
       "3            20,attractions #x2014,395;41,parks,952;                None   \n",
       "4              5,lakh Delhi government employees,47;                None   \n",
       "\n",
       "                                         V2EXTRASXML CountType     Region  \\\n",
       "0                                               None      None       None   \n",
       "1                    <PAGE_AUTHORS>By</PAGE_AUTHORS>      None  Hindustan   \n",
       "2  <PAGE_AUTHORS>Ren&eacute;e Jean rjean@willisto...      KILL   Colorado   \n",
       "3  <PAGE_LINKS>http://cpw.state.co.us/aboutus/Pag...      None       None   \n",
       "4  <PAGE_LINKS>https://www.aninews.in/search</PAG...      None      Delhi   \n",
       "\n",
       "         Country                  THEMES_SINGLE  \n",
       "0        Germany             TAX_WORLDLANGUAGES  \n",
       "1          India                     TAX_FNCACT  \n",
       "2  United States  WB_507_ENERGY_AND_EXTRACTIVES  \n",
       "3  United States                         LEADER  \n",
       "4          India             GENERAL_GOVERNMENT  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b09e03c6-2244-4b57-b227-93b0fdb07579",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T16:25:59.570872Z",
     "iopub.status.busy": "2024-05-05T16:25:59.570221Z",
     "iopub.status.idle": "2024-05-05T16:25:59.628633Z",
     "shell.execute_reply": "2024-05-05T16:25:59.626581Z",
     "shell.execute_reply.started": "2024-05-05T16:25:59.570814Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create separate dataframes for PH and SEA countries\n",
    "df_ph = (df_final\n",
    "         .filter(F.col('Country').rlike('Philippines'))\n",
    "        )\n",
    "\n",
    "sea_countries = [\"Brunei\", \"Cambodia\", \"Indonesia\", \"Laos\", \"Malaysia\",\n",
    "                 \"Myanmar\", \"Singapore\", \"Thailand\", \"Vietnam\",\n",
    "                 \"East Timor\"]\n",
    "\n",
    "# Filtering the DataFrame for SEA countries\n",
    "df_sea = (df_final\n",
    "          .filter(F.col('Country').isin(sea_countries))\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d997c52-7a1b-44ab-b538-c7580439d1ef",
   "metadata": {},
   "source": [
    "# **How many different types of counts were made (e.g., how many arrests, protest)**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "021ed3b8-2458-465f-8fa6-0cd170f68b2b",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-05-05T16:26:09.402136Z",
     "iopub.status.busy": "2024-05-05T16:26:09.401439Z",
     "iopub.status.idle": "2024-05-05T16:26:13.955564Z",
     "shell.execute_reply": "2024-05-05T16:26:13.952935Z",
     "shell.execute_reply.started": "2024-05-05T16:26:09.402076Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o330.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 38 in stage 21.0 failed 1 times, most recent failure: Lost task 38.0 in stage 21.0 (TID 1575) (localhost executor driver): org.apache.spark.SparkFileNotFoundException: File file:/mnt/data/public/gdeltv2/gkg/20190802154500.gkg.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/mnt/data/public/gdeltv2/gkg/20190802154500.gkg.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[43m(\u001b[49m\u001b[43mdf_ph\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCountType\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morderBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mascending\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m----> 5\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/conversion.py:202\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    204\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\n\u001b[1;32m    205\u001b[0m         rows, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rows)), columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:1261\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1241\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m \n\u001b[1;32m   1243\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1261\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o330.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 38 in stage 21.0 failed 1 times, most recent failure: Lost task 38.0 in stage 21.0 (TID 1575) (localhost executor driver): org.apache.spark.SparkFileNotFoundException: File file:/mnt/data/public/gdeltv2/gkg/20190802154500.gkg.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/mnt/data/public/gdeltv2/gkg/20190802154500.gkg.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "# Getting an error here idk why\n",
    "(df_ph\n",
    " .groupBy('CountType')\n",
    " .count()\n",
    " .orderBy('count', ascending=False)\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c5ba25-df80-413b-afac-f345e8b36ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
